{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchNet S2EF training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate some of the basics of the Open Catalyst Project's (OCP) codebase and data. In this example, we will train a schnet model for predicting the energy and forces of a given structure (S2EF task). First, ensure you have installed the OCP ocp repo and all the dependencies according to the [README](https://github.com/Open-Catalyst-Project/ocp/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: This notebook is for tutorial purposes, it is unlikely it will be practical to train baseline models on our larger datasets using this format. As a next step, we recommend trying the command line examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\models\\scn\\spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd = torch.load(os.path.join(os.path.dirname(__file__), \"Jd.pt\"))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "sys.path.append(r\"D:\\Train_thử\\fairchem-tio2-s2ef\")\n",
    "\n",
    "from ocpmodels.trainers import ForcesTrainer\n",
    "from ocpmodels import models\n",
    "from ocpmodels.common import logger\n",
    "from ocpmodels.common.utils import setup_logging\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# a simple sanity check that a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The essential steps for training an OCP model\n",
    "\n",
    "1) Download data\n",
    "\n",
    "2) Preprocess data (if necessary)\n",
    "\n",
    "3) Define or load a configuration (config), which includes the following\n",
    "   \n",
    "   - task\n",
    "   - model\n",
    "   - optimizer\n",
    "   - dataset\n",
    "   - trainer\n",
    "\n",
    "4) Train\n",
    "\n",
    "5) Depending on the model/task there might be intermediate relaxation step\n",
    "\n",
    "6) Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples uses the LMDB generated from the following [tutorial](http://laikapack.cheme.cmu.edu/notebook/open-catalyst-project/mshuaibi/notebooks/projects/ocp/docs/source/tutorials/lmdb_dataset_creation.ipynb). Please run that notebook before moving on. Alternatively, if you have other LMDBs available you may specify that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to your local lmdb directory\n",
    "# train_src = \"s2ef\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3550: UserWarning: TrajectoryLmdbDataset is deprecated and will be removed in the future.Please use 'LmdbDataset' instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from ocpmodels.datasets import TrajectoryLmdbDataset\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "train_src = \"s2ef\"\n",
    "\n",
    "train_dataset = TrajectoryLmdbDataset({\"src\": train_src})\n",
    "\n",
    "energies = []\n",
    "for data in train_dataset:\n",
    "  energies.append(data.y)\n",
    "\n",
    "mean = np.mean(energies)\n",
    "stdev = np.std(energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will explicitly define the config; however, a set of default config files exists in the config folder of this repository. Default config yaml files can easily be loaded with the `build_config` util (found in `ocp/ocpmodels/common/utils.py`). Loading a yaml config is preferrable when launching jobs from the command line. We have included our best models' config files [here](https://github.com/Open-Catalyst-Project/ocp/tree/master/configs/s2ef)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    'dataset': 'trajectory_lmdb', # dataset used for the S2EF task\n",
    "    'description': 'Regressing to energies and forces for DFT trajectories from OCP',\n",
    "    'type': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'labels': ['potential energy'],\n",
    "    'grad_input': 'atomic forces',\n",
    "    'train_on_free_atoms': True,\n",
    "    'eval_on_free_atoms': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model** - SchNet for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'name': 'gemnet_t',\n",
    "    \"num_spherical\": 7,\n",
    "    \"num_radial\": 128,\n",
    "    \"num_blocks\": 3,\n",
    "    \"emb_size_atom\": 512,\n",
    "    \"emb_size_edge\": 512,\n",
    "    \"emb_size_trip\": 64,\n",
    "    \"emb_size_rbf\": 16,\n",
    "    \"emb_size_cbf\": 16,\n",
    "    \"emb_size_bil_trip\": 64,\n",
    "    \"num_before_skip\": 1,\n",
    "    \"num_after_skip\": 2,\n",
    "    \"num_concat\": 1,\n",
    "    \"num_atom\": 3,\n",
    "    \"cutoff\": 6.0,\n",
    "    \"max_neighbors\": 50,\n",
    "    \"rbf\": {\"name\": \"gaussian\"},\n",
    "    \"envelope\": {\n",
    "      \"name\": \"polynomial\",\n",
    "      \"exponent\": 5,\n",
    "    },\n",
    "    \"cbf\": {\"name\": \"spherical_harmonics\"},\n",
    "    \"extensive\": True,\n",
    "    \"otf_graph\": True,\n",
    "    \"output_init\": \"HeOrthogonal\",\n",
    "    \"activation\": \"silu\",\n",
    "    \"scale_file\": \"D:\\\\Train_thử\\\\fairchem-tio2-s2ef\\\\configs\\\\s2ef\\\\all\\\\gemnet\\\\scaling_factors\\\\gemnet-dT.json\",\n",
    "    \"direct_forces\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = {\n",
    "    'batch_size': 1,         # originally 32\n",
    "    'eval_batch_size': 1,    # originally 32\n",
    "    'num_workers': 0,\n",
    "    'lr_initial': 5.e-4,\n",
    "    'optimizer': 'AdamW',\n",
    "    'optimizer_params': {\"amsgrad\": True},\n",
    "    'scheduler': \"ReduceLROnPlateau\",\n",
    "    'mode': \"min\",\n",
    "    'factor': 0.8,\n",
    "    'patience': 3,\n",
    "    'max_epochs': 1,         # used for demonstration purposes\n",
    "    'force_coefficient': 100,\n",
    "    'ema_decay': 0.999,\n",
    "    'clip_grad_norm': 10,\n",
    "    'loss_energy': 'mae',\n",
    "    'loss_force': 'l2mae',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, `train_src` is used for all the train/val/test sets. Feel free to update with the actual S2EF val and test sets, but it does require additional downloads and preprocessing. If you desire to normalize your targets, `normalize_labels` must be set to `True` and corresponding `mean` and `stds` need to be specified. These values have been precomputed for you and can be found in any of the [`base.yml`](https://github.com/Open-Catalyst-Project/ocp/blob/master/configs/s2ef/20M/base.yml#L5-L9) config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = [\n",
    "  {'src': train_src,\n",
    "   'normalize_labels': True,\n",
    "   \"target_mean\": mean,\n",
    "   \"target_std\": stdev,\n",
    "   \"grad_target_mean\": 0.0,\n",
    "   \"grad_target_std\": stdev\n",
    "   }, # train set \n",
    "  {'src': train_src},\n",
    "]\n",
    "\n",
    "# dataset = [\n",
    "# {'src': train_src, 'normalize_labels': False}, # train set \n",
    "# {'src': train_src}, # val set (optional)\n",
    "# {'src': train_src} # test set (optional - writes predictions to disk)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ForcesTrainer` for the S2EF and IS2RS tasks, and the `EnergyTrainer` for the IS2RE task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: true\n",
      "cmd:\n",
      "  checkpoint_dir: ./checkpoints\\2026-01-25-10-39-44-S2EF-example\n",
      "  commit: 0ac55b8\n",
      "  identifier: S2EF-example\n",
      "  logs_dir: ./logs\\tensorboard\\2026-01-25-10-39-44-S2EF-example\n",
      "  print_every: 5\n",
      "  results_dir: ./results\\2026-01-25-10-39-44-S2EF-example\n",
      "  seed: 0\n",
      "  timestamp_id: 2026-01-25-10-39-44-S2EF-example\n",
      "dataset:\n",
      "  grad_target_mean: 0.0\n",
      "  grad_target_std: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - &id001 !!python/object/apply:numpy.dtype\n",
      "    args:\n",
      "    - f8\n",
      "    - false\n",
      "    - true\n",
      "    state: !!python/tuple\n",
      "    - 3\n",
      "    - <\n",
      "    - null\n",
      "    - null\n",
      "    - null\n",
      "    - -1\n",
      "    - -1\n",
      "    - 0\n",
      "  - !!binary |\n",
      "    H+KjJnIHbUA=\n",
      "  normalize_labels: true\n",
      "  src: s2ef\n",
      "  target_mean: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - *id001\n",
      "  - !!binary |\n",
      "    XsP4k1ppdsA=\n",
      "  target_std: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - *id001\n",
      "  - !!binary |\n",
      "    H+KjJnIHbUA=\n",
      "gpus: 0\n",
      "logger: tensorboard\n",
      "model: gemnet_t\n",
      "model_attributes:\n",
      "  activation: silu\n",
      "  cbf:\n",
      "    name: spherical_harmonics\n",
      "  cutoff: 6.0\n",
      "  direct_forces: true\n",
      "  emb_size_atom: 512\n",
      "  emb_size_bil_trip: 64\n",
      "  emb_size_cbf: 16\n",
      "  emb_size_edge: 512\n",
      "  emb_size_rbf: 16\n",
      "  emb_size_trip: 64\n",
      "  envelope:\n",
      "    exponent: 5\n",
      "    name: polynomial\n",
      "  extensive: true\n",
      "  max_neighbors: 50\n",
      "  num_after_skip: 2\n",
      "  num_atom: 3\n",
      "  num_before_skip: 1\n",
      "  num_blocks: 3\n",
      "  num_concat: 1\n",
      "  num_radial: 128\n",
      "  num_spherical: 7\n",
      "  otf_graph: true\n",
      "  output_init: HeOrthogonal\n",
      "  rbf:\n",
      "    name: gaussian\n",
      "  regress_forces: true\n",
      "  scale_file: \"C:\\\\Users\\\\admin\\\\Downloads\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\configs\\\\\\\n",
      "    s2ef\\\\all\\\\gemnet\\\\scaling_factors\\\\gemnet-dT.json\"\n",
      "noddp: false\n",
      "optim:\n",
      "  batch_size: 1\n",
      "  clip_grad_norm: 10\n",
      "  ema_decay: 0.999\n",
      "  eval_batch_size: 1\n",
      "  factor: 0.8\n",
      "  force_coefficient: 100\n",
      "  loss_energy: mae\n",
      "  loss_force: l2mae\n",
      "  lr_initial: 0.0005\n",
      "  max_epochs: 1\n",
      "  mode: min\n",
      "  num_workers: 0\n",
      "  optimizer: AdamW\n",
      "  optimizer_params:\n",
      "    amsgrad: true\n",
      "  patience: 3\n",
      "  scheduler: ReduceLROnPlateau\n",
      "slurm: {}\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "trainer: forces\n",
      "val_dataset:\n",
      "  src: s2ef\n",
      "\n",
      "2026-01-25 10:40:35 (INFO): Batch balancing is disabled for single GPU training.\n",
      "2026-01-25 10:40:35 (INFO): Batch balancing is disabled for single GPU training.\n",
      "2026-01-25 10:40:35 (INFO): Loading dataset: trajectory_lmdb\n",
      "2026-01-25 10:40:35 (INFO): Loading model: gemnet_t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\base_trainer.py:154: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if amp else None\n",
      "c:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 10:40:36 (INFO): Loaded GemNetT with 31671825 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 10:40:36 (WARNING): Model gradient logging to tensorboard not yet supported.\n"
     ]
    }
   ],
   "source": [
    "trainer = ForcesTrainer(\n",
    "    task=task,\n",
    "    model=copy.deepcopy(model), # copied for later use, not necessary in practice.\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    identifier=\"S2EF-example\",\n",
    "    run_dir=\"./\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    # is_vis=False,\n",
    "    print_every=5,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=True, # use PyTorch Automatic Mixed Precision (faster training and less memory usage),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCPDataParallel(\n",
      "  (module): GemNetT(\n",
      "    (radial_basis): RadialBasis(\n",
      "      (envelope): PolynomialEnvelope()\n",
      "      (rbf): GaussianSmearing()\n",
      "    )\n",
      "    (cbf_basis3): CircularBasisLayer(\n",
      "      (radial_basis): RadialBasis(\n",
      "        (envelope): PolynomialEnvelope()\n",
      "        (rbf): GaussianSmearing()\n",
      "      )\n",
      "    )\n",
      "    (mlp_rbf3): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "      (_activation): Identity()\n",
      "    )\n",
      "    (mlp_cbf3): EfficientInteractionDownProjection()\n",
      "    (mlp_rbf_h): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "      (_activation): Identity()\n",
      "    )\n",
      "    (mlp_rbf_out): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "      (_activation): Identity()\n",
      "    )\n",
      "    (atom_emb): AtomEmbedding(\n",
      "      (embeddings): Embedding(83, 512)\n",
      "    )\n",
      "    (edge_emb): EdgeEmbedding(\n",
      "      (dense): Dense(\n",
      "        (linear): Linear(in_features=1152, out_features=512, bias=False)\n",
      "        (_activation): ScaledSiLU(\n",
      "          (_activation): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (out_blocks): ModuleList(\n",
      "      (0-3): 4 x OutputBlock(\n",
      "        (dense_rbf): Dense(\n",
      "          (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "        (scale_sum): ScaleFactor()\n",
      "        (layers): ModuleList(\n",
      "          (0): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (seq_energy): ModuleList(\n",
      "          (0): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (out_energy): Dense(\n",
      "          (linear): Linear(in_features=512, out_features=1, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "        (scale_rbf_F): ScaleFactor()\n",
      "        (seq_forces): ModuleList(\n",
      "          (0): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (out_forces): Dense(\n",
      "          (linear): Linear(in_features=512, out_features=1, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "        (dense_rbf_F): Dense(\n",
      "          (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (int_blocks): ModuleList(\n",
      "      (0-2): 3 x InteractionBlockTripletsOnly(\n",
      "        (dense_ca): Dense(\n",
      "          (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (_activation): ScaledSiLU(\n",
      "            (_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (trip_interaction): TripletInteraction(\n",
      "          (dense_ba): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (mlp_rbf): Dense(\n",
      "            (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "            (_activation): Identity()\n",
      "          )\n",
      "          (scale_rbf): ScaleFactor()\n",
      "          (mlp_cbf): EfficientInteractionBilinear()\n",
      "          (scale_cbf_sum): ScaleFactor()\n",
      "          (down_projection): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (up_projection_ca): Dense(\n",
      "            (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (up_projection_ac): Dense(\n",
      "            (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layers_before_skip): ModuleList(\n",
      "          (0): ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layers_after_skip): ModuleList(\n",
      "          (0-1): 2 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (atom_update): AtomUpdateBlock(\n",
      "          (dense_rbf): Dense(\n",
      "            (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "            (_activation): Identity()\n",
      "          )\n",
      "          (scale_sum): ScaleFactor()\n",
      "          (layers): ModuleList(\n",
      "            (0): Dense(\n",
      "              (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (_activation): ScaledSiLU(\n",
      "                (_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "            (1-3): 3 x ResidualLayer(\n",
      "              (dense_mlp): Sequential(\n",
      "                (0): Dense(\n",
      "                  (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                  (_activation): ScaledSiLU(\n",
      "                    (_activation): SiLU()\n",
      "                  )\n",
      "                )\n",
      "                (1): Dense(\n",
      "                  (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                  (_activation): ScaledSiLU(\n",
      "                    (_activation): SiLU()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (concat_layer): EdgeEmbedding(\n",
      "          (dense): Dense(\n",
      "            (linear): Linear(in_features=1536, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (residual_m): ModuleList(\n",
      "          (0): ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Downloads\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\forces_trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.scaler is not None):\n",
      "c:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 10:40:41 (INFO): forcesx_mae: 8.87e+02, forcesy_mae: 1.59e+03, forcesz_mae: 2.54e+03, forces_mae: 1.67e+03, forces_cos: -6.02e-02, forces_magnitude: 3.53e+03, energy_mae: 3.43e+05, energy_force_within_threshold: 0.00e+00, loss: 2.87e+03, lr: 5.00e-04, epoch: 2.00e-04, step: 5.00e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Downloads\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\forces_trainer.py:334\u001b[0m, in \u001b[0;36mForcesTrainer.train\u001b[1;34m(self, disable_eval_tqdm)\u001b[0m\n\u001b[0;32m    332\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(out, batch)\n\u001b[0;32m    333\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[1;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mget_scale() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# Compute metrics.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Downloads\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\base_trainer.py:713\u001b[0m, in \u001b[0;36mBaseTrainer._backward\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_backward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss):\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 713\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;66;03m# Scale down the gradients of shared parameters\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint\n",
    "Once training has completed a `Trainer` class, by default, is loaded with the best checkpoint as determined by training or validation (if available) metrics. To load a `Trainer` class directly with a pretrained model, specify the `checkpoint_path` as defined by your previously trained model (`checkpoint_dir`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/2021-09-04-08-51-28-SchNet-example/checkpoint.pt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(trainer.config[\"cmd\"][\"checkpoint_dir\"], \"checkpoint.pt\")\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: false\n",
      "cmd:\n",
      "  checkpoint_dir: ./checkpoints/2021-09-04-08-51-28-SchNet-example\n",
      "  commit: 98a06d8\n",
      "  identifier: SchNet-example\n",
      "  logs_dir: ./logs/tensorboard/2021-09-04-08-51-28-SchNet-example\n",
      "  print_every: 10\n",
      "  results_dir: ./results/2021-09-04-08-51-28-SchNet-example\n",
      "  seed: 0\n",
      "  timestamp_id: 2021-09-04-08-51-28-SchNet-example\n",
      "dataset:\n",
      "  normalize_labels: false\n",
      "  src: s2ef\n",
      "gpus: 1\n",
      "logger: tensorboard\n",
      "model: schnet\n",
      "model_attributes:\n",
      "  cutoff: 6.0\n",
      "  hidden_channels: 1024\n",
      "  num_filters: 256\n",
      "  num_gaussians: 200\n",
      "  num_interactions: 3\n",
      "optim:\n",
      "  batch_size: 16\n",
      "  eval_batch_size: 8\n",
      "  factor: 0.8\n",
      "  force_coefficient: 100\n",
      "  lr_initial: 0.0001\n",
      "  max_epochs: 1\n",
      "  mode: min\n",
      "  num_workers: 8\n",
      "  patience: 3\n",
      "  scheduler: ReduceLROnPlateau\n",
      "slurm: {}\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "test_dataset:\n",
      "  src: s2ef\n",
      "val_dataset:\n",
      "  src: s2ef\n",
      "\n",
      "2021-09-04 08:51:51 (INFO): Loading dataset: trajectory_lmdb\n",
      "2021-09-04 08:51:51 (INFO): Loading model: schnet\n",
      "2021-09-04 08:51:51 (INFO): Loaded SchNet with 5704193 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-04 08:51:51 (WARNING): Model gradient logging to tensorboard not yet supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-04 08:51:51 (INFO): Loading checkpoint from: ./checkpoints/2021-09-04-08-51-28-SchNet-example/checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    'name': 'schnet',\n",
    "    'hidden_channels': 1024, # if training is too slow for example purposes reduce the number of hidden channels\n",
    "    'num_filters': 256,\n",
    "    'num_interactions': 3,\n",
    "    'num_gaussians': 200,\n",
    "    'cutoff': 6.0\n",
    "}\n",
    "\n",
    "pretrained_trainer = ForcesTrainer(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    identifier=\"SchNet-example\",\n",
    "    run_dir=\"./\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    is_vis=False,\n",
    "    print_every=10,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=False, # use PyTorch Automatic Mixed Precision (faster training and less memory usage)\n",
    ")\n",
    "\n",
    "pretrained_trainer.load_checkpoint(checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a test has been provided in your config, predictions are generated and written to disk automatically upon training completion. Otherwise, to make predictions on unseen data a `torch.utils.data` DataLoader object must be constructed. Here we reference our test set to make predictions on. Predictions are saved in `{results_file}.npz` in your `results_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-04 08:51:51 (INFO): Predicting on test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device 0: 100%|██████████| 79/79 [00:01<00:00, 44.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-04 08:51:53 (INFO): Writing results to ./results/2021-09-04-08-51-28-SchNet-example/s2ef_s2ef_results.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the existing test_loader\n",
    "predictions = pretrained_trainer.predict(pretrained_trainer.test_loader, results_file=\"s2ef_results\", disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = predictions[\"energy\"]\n",
    "forces = predictions[\"forces\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tio2-s2ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
