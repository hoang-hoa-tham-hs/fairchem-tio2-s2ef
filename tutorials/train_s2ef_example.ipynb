{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchNet S2EF training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate some of the basics of the Open Catalyst Project's (OCP) codebase and data. In this example, we will train a schnet model for predicting the energy and forces of a given structure (S2EF task). First, ensure you have installed the OCP ocp repo and all the dependencies according to the [README](https://github.com/Open-Catalyst-Project/ocp/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: This notebook is for tutorial purposes, it is unlikely it will be practical to train baseline models on our larger datasets using this format. As a next step, we recommend trying the command line examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\models\\scn\\spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd = torch.load(os.path.join(os.path.dirname(__file__), \"Jd.pt\"))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "sys.path.append(r\"D:\\Train_thử\\fairchem-tio2-s2ef\")\n",
    "\n",
    "from ocpmodels.trainers import ForcesTrainer\n",
    "from ocpmodels import models\n",
    "from ocpmodels.common import logger\n",
    "from ocpmodels.common.utils import setup_logging\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# a simple sanity check that a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The essential steps for training an OCP model\n",
    "\n",
    "1) Download data\n",
    "\n",
    "2) Preprocess data (if necessary)\n",
    "\n",
    "3) Define or load a configuration (config), which includes the following\n",
    "   \n",
    "   - task\n",
    "   - model\n",
    "   - optimizer\n",
    "   - dataset\n",
    "   - trainer\n",
    "\n",
    "4) Train\n",
    "\n",
    "5) Depending on the model/task there might be intermediate relaxation step\n",
    "\n",
    "6) Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples uses the LMDB generated from the following [tutorial](http://laikapack.cheme.cmu.edu/notebook/open-catalyst-project/mshuaibi/notebooks/projects/ocp/docs/source/tutorials/lmdb_dataset_creation.ipynb). Please run that notebook before moving on. Alternatively, if you have other LMDBs available you may specify that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to your local lmdb directory\n",
    "# train_src = \"s2ef\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3550: UserWarning: TrajectoryLmdbDataset is deprecated and will be removed in the future.Please use 'LmdbDataset' instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from ocpmodels.datasets import TrajectoryLmdbDataset\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "train_src = r\"D:\\Train_thử\\fairchem-tio2-s2ef\\tio2_s2ef\\data\\tio2_lmdb\\200k\"\n",
    "\n",
    "train_dataset = TrajectoryLmdbDataset({\"src\": train_src})\n",
    "\n",
    "energies = []\n",
    "for data in train_dataset:\n",
    "  energies.append(data.y)\n",
    "\n",
    "mean = np.mean(energies)\n",
    "stdev = np.std(energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will explicitly define the config; however, a set of default config files exists in the config folder of this repository. Default config yaml files can easily be loaded with the `build_config` util (found in `ocp/ocpmodels/common/utils.py`). Loading a yaml config is preferrable when launching jobs from the command line. We have included our best models' config files [here](https://github.com/Open-Catalyst-Project/ocp/tree/master/configs/s2ef)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    'dataset': 'trajectory_lmdb', # dataset used for the S2EF task\n",
    "    'description': 'Regressing to energies and forces for DFT trajectories from OCP',\n",
    "    'type': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'labels': ['potential energy'],\n",
    "    'grad_input': 'atomic forces',\n",
    "    'train_on_free_atoms': True,\n",
    "    'eval_on_free_atoms': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model** - SchNet for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'name': 'gemnet_t',\n",
    "    \"num_spherical\": 7,\n",
    "    \"num_radial\": 128,\n",
    "    \"num_blocks\": 3,\n",
    "    \"emb_size_atom\": 512,\n",
    "    \"emb_size_edge\": 512,\n",
    "    \"emb_size_trip\": 64,\n",
    "    \"emb_size_rbf\": 16,\n",
    "    \"emb_size_cbf\": 16,\n",
    "    \"emb_size_bil_trip\": 64,\n",
    "    \"num_before_skip\": 1,\n",
    "    \"num_after_skip\": 2,\n",
    "    \"num_concat\": 1,\n",
    "    \"num_atom\": 3,\n",
    "    \"cutoff\": 6.0,\n",
    "    \"max_neighbors\": 50,\n",
    "    \"rbf\": {\"name\": \"gaussian\"},\n",
    "    \"envelope\": {\n",
    "      \"name\": \"polynomial\",\n",
    "      \"exponent\": 5,\n",
    "    },\n",
    "    \"cbf\": {\"name\": \"spherical_harmonics\"},\n",
    "    \"extensive\": True,\n",
    "    \"otf_graph\": True,\n",
    "    \"output_init\": \"HeOrthogonal\",\n",
    "    \"activation\": \"silu\",\n",
    "    \"scale_file\": \"D:\\\\Train_thử\\\\fairchem-tio2-s2ef\\\\configs\\\\s2ef\\\\all\\\\gemnet\\\\scaling_factors\\\\gemnet-dT.json\",\n",
    "    \"direct_forces\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = {\n",
    "    'batch_size': 8,         # originally 32\n",
    "    'eval_batch_size': 1,    # originally 32\n",
    "    'num_workers': 0,\n",
    "    'lr_initial': 5.e-4,\n",
    "    'optimizer': 'AdamW',\n",
    "    'optimizer_params': {\"amsgrad\": True},\n",
    "    'scheduler': \"ReduceLROnPlateau\",\n",
    "    'mode': \"min\",\n",
    "    'factor': 0.8,\n",
    "    'patience': 3,\n",
    "    'max_epochs': 1,         # used for demonstration purposes\n",
    "    'force_coefficient': 100,\n",
    "    'ema_decay': 0.999,\n",
    "    'clip_grad_norm': 10,\n",
    "    'loss_energy': 'mae',\n",
    "    'loss_force': 'l2mae',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, `train_src` is used for all the train/val/test sets. Feel free to update with the actual S2EF val and test sets, but it does require additional downloads and preprocessing. If you desire to normalize your targets, `normalize_labels` must be set to `True` and corresponding `mean` and `stds` need to be specified. These values have been precomputed for you and can be found in any of the [`base.yml`](https://github.com/Open-Catalyst-Project/ocp/blob/master/configs/s2ef/20M/base.yml#L5-L9) config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = [\n",
    "  {'src': train_src,\n",
    "   'normalize_labels': True,\n",
    "   \"target_mean\": mean,\n",
    "   \"target_std\": stdev,\n",
    "   \"grad_target_mean\": 0.0,\n",
    "   \"grad_target_std\": stdev\n",
    "   }, # train set \n",
    "  {'src': train_src},\n",
    "]\n",
    "\n",
    "# dataset = [\n",
    "# {'src': train_src, 'normalize_labels': False}, # train set \n",
    "# {'src': train_src}, # val set (optional)\n",
    "# {'src': train_src} # test set (optional - writes predictions to disk)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ForcesTrainer` for the S2EF and IS2RS tasks, and the `EnergyTrainer` for the IS2RE task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: true\n",
      "cmd:\n",
      "  checkpoint_dir: ./checkpoints\\2026-01-31-16-16-48-S2EF-example\n",
      "  commit: 5571d3b\n",
      "  identifier: S2EF-example\n",
      "  logs_dir: ./logs\\tensorboard\\2026-01-31-16-16-48-S2EF-example\n",
      "  print_every: 5\n",
      "  results_dir: ./results\\2026-01-31-16-16-48-S2EF-example\n",
      "  seed: 0\n",
      "  timestamp_id: 2026-01-31-16-16-48-S2EF-example\n",
      "dataset:\n",
      "  grad_target_mean: 0.0\n",
      "  grad_target_std: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - &id001 !!python/object/apply:numpy.dtype\n",
      "    args:\n",
      "    - f8\n",
      "    - false\n",
      "    - true\n",
      "    state: !!python/tuple\n",
      "    - 3\n",
      "    - <\n",
      "    - null\n",
      "    - null\n",
      "    - null\n",
      "    - -1\n",
      "    - -1\n",
      "    - 0\n",
      "  - !!binary |\n",
      "    3TJ86Am2bEA=\n",
      "  normalize_labels: true\n",
      "  src: \"D:\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\tio2_s2ef\\\\data\\\\tio2_lmdb\\\\200k\"\n",
      "  target_mean: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - *id001\n",
      "  - !!binary |\n",
      "    GubjwdEPhcA=\n",
      "  target_std: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - *id001\n",
      "  - !!binary |\n",
      "    3TJ86Am2bEA=\n",
      "gpus: 0\n",
      "logger: tensorboard\n",
      "model: gemnet_t\n",
      "model_attributes:\n",
      "  activation: silu\n",
      "  cbf:\n",
      "    name: spherical_harmonics\n",
      "  cutoff: 6.0\n",
      "  direct_forces: true\n",
      "  emb_size_atom: 512\n",
      "  emb_size_bil_trip: 64\n",
      "  emb_size_cbf: 16\n",
      "  emb_size_edge: 512\n",
      "  emb_size_rbf: 16\n",
      "  emb_size_trip: 64\n",
      "  envelope:\n",
      "    exponent: 5\n",
      "    name: polynomial\n",
      "  extensive: true\n",
      "  max_neighbors: 50\n",
      "  num_after_skip: 2\n",
      "  num_atom: 3\n",
      "  num_before_skip: 1\n",
      "  num_blocks: 3\n",
      "  num_concat: 1\n",
      "  num_radial: 128\n",
      "  num_spherical: 7\n",
      "  otf_graph: true\n",
      "  output_init: HeOrthogonal\n",
      "  rbf:\n",
      "    name: gaussian\n",
      "  scale_file: \"D:\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\configs\\\\s2ef\\\\all\\\\gemnet\\\\\\\n",
      "    scaling_factors\\\\gemnet-dT.json\"\n",
      "noddp: false\n",
      "optim:\n",
      "  batch_size: 8\n",
      "  clip_grad_norm: 10\n",
      "  ema_decay: 0.999\n",
      "  eval_batch_size: 1\n",
      "  factor: 0.8\n",
      "  force_coefficient: 100\n",
      "  loss_energy: mae\n",
      "  loss_force: l2mae\n",
      "  lr_initial: 0.0005\n",
      "  max_epochs: 1\n",
      "  mode: min\n",
      "  num_workers: 0\n",
      "  optimizer: AdamW\n",
      "  optimizer_params:\n",
      "    amsgrad: true\n",
      "  patience: 3\n",
      "  scheduler: ReduceLROnPlateau\n",
      "slurm: {}\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "trainer: forces\n",
      "val_dataset:\n",
      "  src: \"D:\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\tio2_s2ef\\\\data\\\\tio2_lmdb\\\\200k\"\n",
      "\n",
      "2026-01-31 16:17:32 (INFO): Batch balancing is disabled for single GPU training.\n",
      "2026-01-31 16:17:32 (INFO): Batch balancing is disabled for single GPU training.\n",
      "2026-01-31 16:17:32 (INFO): Loading dataset: trajectory_lmdb\n",
      "2026-01-31 16:17:32 (INFO): Loading model: gemnet_t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\base_trainer.py:154: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if amp else None\n",
      "c:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:17:34 (INFO): Loaded GemNetT with 31671825 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:17:34 (WARNING): Model gradient logging to tensorboard not yet supported.\n"
     ]
    }
   ],
   "source": [
    "trainer = ForcesTrainer(\n",
    "    task=task,\n",
    "    model=copy.deepcopy(model), # copied for later use, not necessary in practice.\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    identifier=\"S2EF-example\",\n",
    "    run_dir=\"./\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    # is_vis=False,\n",
    "    print_every=5,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=True, # use PyTorch Automatic Mixed Precision (faster training and less memory usage),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCPDataParallel(\n",
      "  (module): GemNetT(\n",
      "    (radial_basis): RadialBasis(\n",
      "      (envelope): PolynomialEnvelope()\n",
      "      (rbf): GaussianSmearing()\n",
      "    )\n",
      "    (cbf_basis3): CircularBasisLayer(\n",
      "      (radial_basis): RadialBasis(\n",
      "        (envelope): PolynomialEnvelope()\n",
      "        (rbf): GaussianSmearing()\n",
      "      )\n",
      "    )\n",
      "    (mlp_rbf3): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "      (_activation): Identity()\n",
      "    )\n",
      "    (mlp_cbf3): EfficientInteractionDownProjection()\n",
      "    (mlp_rbf_h): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "      (_activation): Identity()\n",
      "    )\n",
      "    (mlp_rbf_out): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "      (_activation): Identity()\n",
      "    )\n",
      "    (atom_emb): AtomEmbedding(\n",
      "      (embeddings): Embedding(83, 512)\n",
      "    )\n",
      "    (edge_emb): EdgeEmbedding(\n",
      "      (dense): Dense(\n",
      "        (linear): Linear(in_features=1152, out_features=512, bias=False)\n",
      "        (_activation): ScaledSiLU(\n",
      "          (_activation): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (out_blocks): ModuleList(\n",
      "      (0-3): 4 x OutputBlock(\n",
      "        (dense_rbf): Dense(\n",
      "          (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "        (scale_sum): ScaleFactor()\n",
      "        (layers): ModuleList(\n",
      "          (0): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (seq_energy): ModuleList(\n",
      "          (0): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (out_energy): Dense(\n",
      "          (linear): Linear(in_features=512, out_features=1, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "        (scale_rbf_F): ScaleFactor()\n",
      "        (seq_forces): ModuleList(\n",
      "          (0): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (out_forces): Dense(\n",
      "          (linear): Linear(in_features=512, out_features=1, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "        (dense_rbf_F): Dense(\n",
      "          (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "          (_activation): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (int_blocks): ModuleList(\n",
      "      (0-2): 3 x InteractionBlockTripletsOnly(\n",
      "        (dense_ca): Dense(\n",
      "          (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (_activation): ScaledSiLU(\n",
      "            (_activation): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (trip_interaction): TripletInteraction(\n",
      "          (dense_ba): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (mlp_rbf): Dense(\n",
      "            (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "            (_activation): Identity()\n",
      "          )\n",
      "          (scale_rbf): ScaleFactor()\n",
      "          (mlp_cbf): EfficientInteractionBilinear()\n",
      "          (scale_cbf_sum): ScaleFactor()\n",
      "          (down_projection): Dense(\n",
      "            (linear): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (up_projection_ca): Dense(\n",
      "            (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (up_projection_ac): Dense(\n",
      "            (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layers_before_skip): ModuleList(\n",
      "          (0): ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layers_after_skip): ModuleList(\n",
      "          (0-1): 2 x ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (atom_update): AtomUpdateBlock(\n",
      "          (dense_rbf): Dense(\n",
      "            (linear): Linear(in_features=16, out_features=512, bias=False)\n",
      "            (_activation): Identity()\n",
      "          )\n",
      "          (scale_sum): ScaleFactor()\n",
      "          (layers): ModuleList(\n",
      "            (0): Dense(\n",
      "              (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (_activation): ScaledSiLU(\n",
      "                (_activation): SiLU()\n",
      "              )\n",
      "            )\n",
      "            (1-3): 3 x ResidualLayer(\n",
      "              (dense_mlp): Sequential(\n",
      "                (0): Dense(\n",
      "                  (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                  (_activation): ScaledSiLU(\n",
      "                    (_activation): SiLU()\n",
      "                  )\n",
      "                )\n",
      "                (1): Dense(\n",
      "                  (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                  (_activation): ScaledSiLU(\n",
      "                    (_activation): SiLU()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (concat_layer): EdgeEmbedding(\n",
      "          (dense): Dense(\n",
      "            (linear): Linear(in_features=1536, out_features=512, bias=False)\n",
      "            (_activation): ScaledSiLU(\n",
      "              (_activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (residual_m): ModuleList(\n",
      "          (0): ResidualLayer(\n",
      "            (dense_mlp): Sequential(\n",
      "              (0): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "              (1): Dense(\n",
      "                (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (_activation): ScaledSiLU(\n",
      "                  (_activation): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\forces_trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.scaler is not None):\n",
      "c:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:19:12 (INFO): forcesx_mae: 1.13e+03, forcesy_mae: 2.51e+03, forcesz_mae: 5.32e+03, forces_mae: 2.99e+03, forces_cos: -3.22e-02, forces_magnitude: 6.61e+03, energy_mae: 9.60e+05, energy_force_within_threshold: 0.00e+00, loss: 6.69e+03, lr: 5.00e-04, epoch: 2.27e-01, step: 5.00e+00\n",
      "2026-01-31 16:20:31 (INFO): forcesx_mae: 3.09e+02, forcesy_mae: 4.68e+02, forcesz_mae: 8.31e+02, forces_mae: 5.36e+02, forces_cos: 4.07e-02, forces_magnitude: 1.16e+03, energy_mae: 2.52e+04, energy_force_within_threshold: 0.00e+00, loss: 5.95e+02, lr: 5.00e-04, epoch: 4.55e-01, step: 1.00e+01\n",
      "2026-01-31 16:22:09 (INFO): forcesx_mae: 1.49e+02, forcesy_mae: 2.18e+02, forcesz_mae: 3.18e+02, forces_mae: 2.28e+02, forces_cos: 2.37e-02, forces_magnitude: 4.86e+02, energy_mae: 8.93e+03, energy_force_within_threshold: 0.00e+00, loss: 2.52e+02, lr: 5.00e-04, epoch: 6.82e-01, step: 1.50e+01\n",
      "2026-01-31 16:23:34 (INFO): forcesx_mae: 1.38e+02, forcesy_mae: 1.91e+02, forcesz_mae: 2.37e+02, forces_mae: 1.89e+02, forces_cos: -1.64e-02, forces_magnitude: 3.91e+02, energy_mae: 6.16e+03, energy_force_within_threshold: 0.00e+00, loss: 1.93e+02, lr: 5.00e-04, epoch: 9.09e-01, step: 2.00e+01\n",
      "2026-01-31 16:23:53 (INFO): Evaluating on val.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device 0:   0%|          | 0/169 [00:00<?, ?it/s]D:\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\base_trainer.py:661: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.scaler is not None):\n",
      "device 0: 100%|██████████| 169/169 [01:32<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:25:26 (INFO): forcesx_mae: 301.9076, forcesy_mae: 434.7267, forcesz_mae: 620.1185, forces_mae: 452.2509, forces_cos: -0.0074, forces_magnitude: 951.5119, energy_mae: 34734.3007, energy_force_within_threshold: 0.0000, loss: 580.5457, epoch: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint\n",
    "Once training has completed a `Trainer` class, by default, is loaded with the best checkpoint as determined by training or validation (if available) metrics. To load a `Trainer` class directly with a pretrained model, specify the `checkpoint_path` as defined by your previously trained model (`checkpoint_dir`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints\\\\2026-01-31-16-16-48-S2EF-example\\\\checkpoint.pt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(trainer.config[\"cmd\"][\"checkpoint_dir\"], \"checkpoint.pt\")\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: false\n",
      "cmd:\n",
      "  checkpoint_dir: ./checkpoints\\2026-01-31-16-29-36-SchNet-example\n",
      "  commit: 5571d3b\n",
      "  identifier: SchNet-example\n",
      "  logs_dir: ./logs\\tensorboard\\2026-01-31-16-29-36-SchNet-example\n",
      "  print_every: 10\n",
      "  results_dir: ./results\\2026-01-31-16-29-36-SchNet-example\n",
      "  seed: 0\n",
      "  timestamp_id: 2026-01-31-16-29-36-SchNet-example\n",
      "dataset:\n",
      "  grad_target_mean: 0.0\n",
      "  grad_target_std: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - &id001 !!python/object/apply:numpy.dtype\n",
      "    args:\n",
      "    - f8\n",
      "    - false\n",
      "    - true\n",
      "    state: !!python/tuple\n",
      "    - 3\n",
      "    - <\n",
      "    - null\n",
      "    - null\n",
      "    - null\n",
      "    - -1\n",
      "    - -1\n",
      "    - 0\n",
      "  - !!binary |\n",
      "    3TJ86Am2bEA=\n",
      "  normalize_labels: true\n",
      "  src: \"D:\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\tio2_s2ef\\\\data\\\\tio2_lmdb\\\\200k\"\n",
      "  target_mean: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - *id001\n",
      "  - !!binary |\n",
      "    GubjwdEPhcA=\n",
      "  target_std: !!python/object/apply:numpy._core.multiarray.scalar\n",
      "  - *id001\n",
      "  - !!binary |\n",
      "    3TJ86Am2bEA=\n",
      "gpus: 0\n",
      "logger: tensorboard\n",
      "model: gemnet_t\n",
      "model_attributes:\n",
      "  activation: silu\n",
      "  cbf:\n",
      "    name: spherical_harmonics\n",
      "  cutoff: 6.0\n",
      "  direct_forces: true\n",
      "  emb_size_atom: 512\n",
      "  emb_size_bil_trip: 64\n",
      "  emb_size_cbf: 16\n",
      "  emb_size_edge: 512\n",
      "  emb_size_rbf: 16\n",
      "  emb_size_trip: 64\n",
      "  envelope:\n",
      "    exponent: 5\n",
      "    name: polynomial\n",
      "  extensive: true\n",
      "  max_neighbors: 50\n",
      "  num_after_skip: 2\n",
      "  num_atom: 3\n",
      "  num_before_skip: 1\n",
      "  num_blocks: 3\n",
      "  num_concat: 1\n",
      "  num_radial: 128\n",
      "  num_spherical: 7\n",
      "  otf_graph: true\n",
      "  output_init: HeOrthogonal\n",
      "  rbf:\n",
      "    name: gaussian\n",
      "  scale_file: \"D:\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\configs\\\\s2ef\\\\all\\\\gemnet\\\\\\\n",
      "    scaling_factors\\\\gemnet-dT.json\"\n",
      "noddp: false\n",
      "optim:\n",
      "  batch_size: 8\n",
      "  clip_grad_norm: 10\n",
      "  ema_decay: 0.999\n",
      "  eval_batch_size: 1\n",
      "  factor: 0.8\n",
      "  force_coefficient: 100\n",
      "  loss_energy: mae\n",
      "  loss_force: l2mae\n",
      "  lr_initial: 0.0005\n",
      "  max_epochs: 1\n",
      "  mode: min\n",
      "  num_workers: 0\n",
      "  optimizer: AdamW\n",
      "  optimizer_params:\n",
      "    amsgrad: true\n",
      "  patience: 3\n",
      "  scheduler: ReduceLROnPlateau\n",
      "slurm: {}\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "trainer: forces\n",
      "val_dataset:\n",
      "  src: \"D:\\\\Train_th\\u1EED\\\\fairchem-tio2-s2ef\\\\tio2_s2ef\\\\data\\\\tio2_lmdb\\\\200k\"\n",
      "\n",
      "2026-01-31 16:29:42 (INFO): Batch balancing is disabled for single GPU training.\n",
      "2026-01-31 16:29:42 (INFO): Batch balancing is disabled for single GPU training.\n",
      "2026-01-31 16:29:42 (INFO): Loading dataset: trajectory_lmdb\n",
      "2026-01-31 16:29:42 (INFO): Loading model: gemnet_t\n",
      "2026-01-31 16:29:43 (INFO): Loaded GemNetT with 31671825 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:29:43 (WARNING): Model gradient logging to tensorboard not yet supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:29:43 (INFO): Loading checkpoint from: ./checkpoints\\2026-01-31-16-16-48-S2EF-example\\checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\base_trainer.py:407: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "# model = {\n",
    "#     'name': 'schnet',\n",
    "#     'hidden_channels': 1024, # if training is too slow for example purposes reduce the number of hidden channels\n",
    "#     'num_filters': 256,\n",
    "#     'num_interactions': 3,\n",
    "#     'num_gaussians': 200,\n",
    "#     'cutoff': 6.0\n",
    "# }\n",
    "\n",
    "pretrained_trainer = ForcesTrainer(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    identifier=\"SchNet-example\",\n",
    "    run_dir=\"./\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    # is_vis=False,\n",
    "    print_every=10,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=False, # use PyTorch Automatic Mixed Precision (faster training and less memory usage)\n",
    ")\n",
    "\n",
    "pretrained_trainer.load_checkpoint(checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a test has been provided in your config, predictions are generated and written to disk automatically upon training completion. Otherwise, to make predictions on unseen data a `torch.utils.data` DataLoader object must be constructed. Here we reference our test set to make predictions on. Predictions are saved in `{results_file}.npz` in your `results_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 16:37:29 (INFO): Predicting on test.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# make predictions on the existing test_loader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms2ef_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\tio2-s2ef\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Train_thử\\fairchem-tio2-s2ef\\ocpmodels\\trainers\\forces_trainer.py:163\u001b[0m, in \u001b[0;36mForcesTrainer.predict\u001b[1;34m(self, data_loader, per_image, results_file, disable_tqdm)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m distutils\u001b[38;5;241m.\u001b[39mis_master() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m disable_tqdm:\n\u001b[0;32m    162\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting on test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    164\u001b[0m     data_loader,\n\u001b[0;32m    165\u001b[0m     (\n\u001b[0;32m    166\u001b[0m         torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdataloader\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[0;32m    167\u001b[0m         torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBatch,\n\u001b[0;32m    168\u001b[0m     ),\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    170\u001b[0m rank \u001b[38;5;241m=\u001b[39m distutils\u001b[38;5;241m.\u001b[39mget_rank()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_loader, torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBatch):\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make predictions on the existing test_loader\n",
    "predictions = pretrained_trainer.predict(pretrained_trainer.test_loader, results_file=\"s2ef_results\", disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = predictions[\"energy\"]\n",
    "forces = predictions[\"forces\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tio2-s2ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
